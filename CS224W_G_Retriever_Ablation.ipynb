{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "ewEzT2J0ADYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6aovelV9ylf",
        "outputId": "3290d21b-044d-481f-bbcf-d66fddb7d330"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uv\n",
            "  Downloading uv-0.9.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.9.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.9.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/wenovus/knowledge-rag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IgFEnPi-qwM",
        "outputId": "787e0125-1ef4-46ef-b910-b77bdd0cfde8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'knowledge-rag'...\n",
            "remote: Enumerating objects: 402, done.\u001b[K\n",
            "remote: Counting objects: 100% (127/127), done.\u001b[K\n",
            "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
            "remote: Total 402 (delta 61), reused 82 (delta 29), pack-reused 275 (from 1)\u001b[K\n",
            "Receiving objects: 100% (402/402), 51.48 MiB | 37.23 MiB/s, done.\n",
            "Resolving deltas: 100% (189/189), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/knowledge-rag\n",
        "!./setup.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1fMaolC-JSn",
        "outputId": "c5f5e05f-0bc2-4ff6-822d-2f8af7793234"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/knowledge-rag\n",
            "Requirement already satisfied: uv in /usr/local/lib/python3.12/dist-packages (0.9.8)\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\u001b[0m\n",
            "Using CPython 3.12.12 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `extra-build-dependencies` option is experimental and may change without warning. Pass `--preview-features extra-build-dependencies` to disable this warning.\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 319ms\u001b[0m\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `extra-build-dependencies` option is experimental and may change without warning. Pass `--preview-features extra-build-dependencies` to disable this warning.\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 504ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 130ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-scatter\u001b[0m\u001b[2m==2.1.2+pt26cu124\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `extra-build-dependencies` option is experimental and may change without warning. Pass `--preview-features extra-build-dependencies` to disable this warning.\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m3 packages\u001b[0m \u001b[2min 434ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 65ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-sparse\u001b[0m\u001b[2m==0.6.18+pt26cu124\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `extra-build-dependencies` option is experimental and may change without warning. Pass `--preview-features extra-build-dependencies` to disable this warning.\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m7 packages\u001b[0m \u001b[2min 1.80s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 105ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyg-lib\u001b[0m\u001b[2m==0.5.0+pt26cu124\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-cluster\u001b[0m\u001b[2m==1.6.3+pt26cu124\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-spline-conv\u001b[0m\u001b[2m==1.2.2+pt26cu124\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `extra-build-dependencies` option is experimental and may change without warning. Pass `--preview-features extra-build-dependencies` to disable this warning.\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m126 packages\u001b[0m \u001b[2min 728ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m43 packages\u001b[0m \u001b[2min 28.36s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m26 packages\u001b[0m \u001b[2min 871ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m43 packages\u001b[0m \u001b[2min 217ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1margcomplete\u001b[0m\u001b[2m==3.6.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcolorama\u001b[0m\u001b[2m==0.4.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcommitizen\u001b[0m\u001b[2m==4.10.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdecli\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdeprecated\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgensim\u001b[0m\u001b[2m==4.4.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==6.17.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==7.4.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlittleutils\u001b[0m\u001b[2m==0.2.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.6.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.6.80\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.0.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.7.77\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.6.85\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mogb\u001b[0m\u001b[2m==1.3.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1moutdated\u001b[0m\u001b[2m==0.2.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.2.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpastel\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpcst-fast\u001b[0m\u001b[2m==1.0.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpoethepoet\u001b[0m\u001b[2m==0.37.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.51\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==18.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==22.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpybind11\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mquestionary\u001b[0m\u001b[2m==2.1.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.12.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0+cu126\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-geometric\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.23.0+cu126\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Authenticate with Hugging Face using the stored token\n",
        "login(token='TOKEN')"
      ],
      "metadata": {
        "id": "Ah7rAE3iEREN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!./preprocess.sh\n",
        "!python -m src.dataset.preprocess.expla_graphs\n",
        "!python -m src.dataset.expla_graphs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g5wfByxBW00",
        "outputId": "70e91b0c-a80b-49e5-c8a9-1b2674273099"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 2766/2766 [00:06<00:00, 435.82it/s]\n",
            "inherit model weights from sentence-transformers/all-roberta-large-v1\n",
            "config.json: 100% 650/650 [00:00<00:00, 3.83MB/s]\n",
            "2025-11-12 05:37:06.354602: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-12 05:37:06.373429: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762925826.395181    7871 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762925826.401708    7871 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762925826.418584    7871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762925826.418614    7871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762925826.418616    7871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762925826.418619    7871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-12 05:37:06.423597: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "model.safetensors: 100% 1.42G/1.42G [00:02<00:00, 546MB/s] \n",
            "tokenizer_config.json: 100% 328/328 [00:00<00:00, 2.32MB/s]\n",
            "vocab.json: 798kB [00:00, 7.34MB/s]\n",
            "merges.txt: 456kB [00:00, 12.9MB/s]\n",
            "tokenizer.json: 1.36MB [00:00, 10.7MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.37MB/s]\n",
            "Encoding graphs...\n",
            "100% 2766/2766 [01:47<00:00, 25.75it/s]\n",
            "# train samples:  1659\n",
            "# val samples:  553\n",
            "# test samples:  554\n",
            "<frozen runpy>:128: RuntimeWarning: 'src.dataset.expla_graphs' found in sys.modules after import of package 'src.dataset', but prior to execution of 'src.dataset.expla_graphs'; this may result in unpredictable behaviour\n",
            "Question: Do argument 1 and argument 2 support or counter each other? Answer in one word in the form of 'support' or 'counter'.\n",
            "\n",
            "Answer:\n",
            "id: 0\n",
            "label: support\n",
            "desc: node_id,node_attr\n",
            "0,cannabis\n",
            "1,marijuana\n",
            "2,legal\n",
            "3,more available\n",
            "4,good thing\n",
            "\n",
            "src,edge_attr,dst\n",
            "0,synonym of,1\n",
            "2,causes,3\n",
            "1,capable of,4\n",
            "4,desires,2\n",
            "\n",
            "graph: Data(x=[5, 1024], edge_index=[2, 4], edge_attr=[4, 1024], num_nodes=5)\n",
            "question: Argument 1: Cannabis should be legal.\n",
            "Argument 2: It's not a bad thing to make marijuana more available.\n",
            "Question: Do argument 1 and argument 2 support or counter each other? Answer in one word in the form of 'support' or 'counter'.\n",
            "\n",
            "Answer:\n",
            "# train: 1659\n",
            "# val: 553\n",
            "# test: 554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5AyIudMK9MK",
        "outputId": "52e2cf75-2c6b-4ee5-bf95-a8957ed64a80"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --dataset expla_graphs --model_name inference_llm --llm_model_name 7b_chat --max_txt_len 0 --seed 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVmpm8gZAHtx",
        "outputId": "b9df867b-1ae2-49d6-b1ac-acaee06ecdc0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-12 05:39:25.082771: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-12 05:39:25.100682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762925965.122331    8688 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762925965.128904    8688 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762925965.145640    8688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762925965.145673    8688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762925965.145676    8688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762925965.145679    8688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-12 05:39:25.150557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlongsail\u001b[0m (\u001b[33mlongsail-stanford-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run 9cm6lsm1 (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/knowledge-rag/wandb/run-20251112_054032-9cm6lsm1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexpla_graphs_inference_llm_seed0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/longsail-stanford-university/project_g_retriever\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/longsail-stanford-university/project_g_retriever/runs/9cm6lsm1\u001b[0m\n",
            "Namespace(model_name='inference_llm', project='project_g_retriever', seed=0, dataset='expla_graphs', lr=1e-05, wd=0.05, patience=2, batch_size=8, grad_steps=2, num_epochs=10, warmup_epochs=1, eval_batch_size=16, llm_model_name='7b_chat', llm_model_path='', llm_frozen='True', llm_num_virtual_tokens=10, output_dir='output', max_txt_len=0, max_new_tokens=32, max_memory=[80], gnn_model_name='gt', gnn_num_layers=4, gnn_in_dim=1024, gnn_hidden_dim=1024, gnn_num_heads=4, gnn_dropout=0.0)\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "CUDA device count: 1\n",
            "Device 0: NVIDIA A100-SXM4-80GB\n",
            "Loading model: inference_llm\n",
            "Loading LLAMA\n",
            "kwargs: {'max_memory': {0: '80GiB'}, 'device_map': 'auto', 'revision': 'main'}\n",
            "tokenizer_config.json: 100% 1.62k/1.62k [00:00<00:00, 7.46MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 974kB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.11MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 5.56MB/s]\n",
            "config.json: 100% 614/614 [00:00<00:00, 3.17MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 33.7MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 2.56M/9.98G [00:01<1:23:04, 2.00MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 481M/9.98G [00:01<00:20, 472MB/s]    \u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 753M/9.98G [00:01<00:14, 647MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 6.23M/3.50G [00:01<15:49, 3.68MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 971M/9.98G [00:01<00:11, 814MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  10% 333M/3.50G [00:01<00:12, 250MB/s]  \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  15% 514M/3.50G [00:02<00:08, 339MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.15G/9.98G [00:02<00:20, 435MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.54G/9.98G [00:02<00:12, 679MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  18% 638M/3.50G [00:03<00:15, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  28% 965M/3.50G [00:03<00:07, 335MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.72G/9.98G [00:04<00:31, 261MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  31% 1.10G/3.50G [00:05<00:12, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  33% 1.17G/3.50G [00:06<00:13, 173MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.84G/9.98G [00:06<00:39, 206MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  35% 1.23G/3.50G [00:07<00:19, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  37% 1.29G/3.50G [00:08<00:25, 86.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.96G/9.98G [00:08<01:12, 111MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.04G/9.98G [00:10<01:30, 87.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  40% 1.39G/3.50G [00:10<00:29, 71.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  41% 1.42G/3.50G [00:11<00:27, 76.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  42% 1.47G/3.50G [00:11<00:26, 76.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.13G/9.98G [00:12<01:34, 83.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.16G/9.98G [00:14<02:11, 59.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  44% 1.53G/3.50G [00:14<00:44, 44.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.20G/9.98G [00:14<02:06, 61.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  44% 1.55G/3.50G [00:14<00:38, 51.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.26G/9.98G [00:14<01:43, 74.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  46% 1.60G/3.50G [00:15<00:30, 61.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.35G/9.98G [00:16<01:56, 65.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  47% 1.63G/3.50G [00:17<00:54, 34.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.37G/9.98G [00:17<02:30, 50.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.41G/9.98G [00:18<02:07, 59.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  49% 1.71G/3.50G [00:18<00:41, 42.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.45G/9.98G [00:18<01:59, 62.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  50% 1.73G/3.50G [00:18<00:36, 48.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.50G/9.98G [00:18<01:38, 75.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.52G/9.98G [00:19<01:33, 80.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  51% 1.80G/3.50G [00:19<00:25, 67.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.57G/9.98G [00:19<01:36, 76.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  53% 1.87G/3.50G [00:19<00:22, 71.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.63G/9.98G [00:20<01:19, 92.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.66G/9.98G [00:20<01:18, 93.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.71G/9.98G [00:20<01:06, 109MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  55% 1.94G/3.50G [00:21<00:27, 57.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.86G/9.98G [00:22<01:00, 118MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  57% 1.99G/3.50G [00:22<00:25, 58.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  59% 2.06G/3.50G [00:22<00:17, 82.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  61% 2.12G/3.50G [00:22<00:12, 110MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  63% 2.20G/3.50G [00:23<00:11, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  64% 2.24G/3.50G [00:23<00:11, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  66% 2.31G/3.50G [00:24<00:08, 134MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  68% 2.38G/3.50G [00:24<00:07, 141MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.92G/9.98G [00:25<02:13, 52.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  69% 2.42G/3.50G [00:25<00:10, 99.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  70% 2.46G/3.50G [00:25<00:11, 94.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.98G/9.98G [00:26<02:20, 49.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.05G/9.98G [00:26<01:50, 62.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  72% 2.52G/3.50G [00:27<00:13, 73.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  73% 2.56G/3.50G [00:27<00:14, 65.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.16G/9.98G [00:27<01:30, 75.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  74% 2.58G/3.50G [00:28<00:12, 70.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.18G/9.98G [00:28<01:27, 77.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  75% 2.62G/3.50G [00:28<00:12, 69.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.28G/9.98G [00:28<01:12, 92.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  77% 2.68G/3.50G [00:29<00:09, 86.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.30G/9.98G [00:29<01:21, 82.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.34G/9.98G [00:29<01:09, 94.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.40G/9.98G [00:29<00:52, 126MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  79% 2.75G/3.50G [00:30<00:08, 84.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  80% 2.79G/3.50G [00:30<00:07, 88.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.43G/9.98G [00:30<01:27, 75.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  81% 2.82G/3.50G [00:30<00:08, 75.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  83% 2.91G/3.50G [00:31<00:05, 103MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.46G/9.98G [00:31<01:43, 62.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  84% 2.93G/3.50G [00:31<00:06, 89.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.53G/9.98G [00:32<01:19, 80.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.56G/9.98G [00:32<01:11, 89.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.60G/9.98G [00:32<01:15, 84.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  86% 3.00G/3.50G [00:32<00:06, 79.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.63G/9.98G [00:32<01:01, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.67G/9.98G [00:34<01:41, 62.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  87% 3.03G/3.50G [00:34<00:10, 45.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.70G/9.98G [00:35<01:54, 55.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  88% 3.10G/3.50G [00:35<00:07, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  89% 3.13G/3.50G [00:35<00:06, 60.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.78G/9.98G [00:36<01:46, 58.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  90% 3.16G/3.50G [00:36<00:05, 64.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.81G/9.98G [00:36<01:50, 56.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.83G/9.98G [00:37<02:01, 50.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.86G/9.98G [00:38<02:26, 41.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  92% 3.23G/3.50G [00:38<00:06, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  94% 3.30G/3.50G [00:38<00:03, 64.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.98G/9.98G [00:38<01:11, 84.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  96% 3.36G/3.50G [00:39<00:01, 79.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 4.01G/9.98G [00:39<01:22, 72.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 4.03G/9.98G [00:39<01:14, 79.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.07G/9.98G [00:40<01:11, 82.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  97% 3.39G/3.50G [00:40<00:01, 57.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.10G/9.98G [00:40<01:30, 65.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  99% 3.45G/3.50G [00:41<00:00, 63.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.14G/9.98G [00:41<01:13, 79.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.18G/9.98G [00:41<01:26, 67.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.20G/9.98G [00:42<01:23, 69.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.27G/9.98G [00:42<00:58, 97.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.28G/9.98G [00:42<00:58, 97.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.33G/9.98G [00:42<00:41, 135MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.40G/9.98G [00:43<00:35, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.43G/9.98G [00:43<00:35, 158MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.51G/9.98G [00:43<00:26, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.54G/9.98G [00:43<00:26, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.56G/9.98G [00:44<00:48, 112MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.59G/9.98G [00:44<00:54, 98.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors: 100% 3.50G/3.50G [00:45<00:00, 77.7MB/s]\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 4.67G/9.98G [00:46<01:31, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.74G/9.98G [00:47<01:03, 82.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.76G/9.98G [00:47<01:01, 84.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.80G/9.98G [00:48<01:10, 73.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.83G/9.98G [00:48<01:20, 63.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.89G/9.98G [00:48<00:55, 91.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.95G/9.98G [00:49<00:53, 93.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 5.01G/9.98G [00:49<00:39, 127MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.04G/9.98G [00:50<00:41, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.13G/9.98G [00:50<00:31, 156MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.21G/9.98G [00:50<00:22, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.27G/9.98G [00:50<00:20, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.32G/9.98G [00:51<00:34, 136MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.38G/9.98G [00:51<00:27, 167MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.45G/9.98G [00:51<00:22, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.48G/9.98G [00:52<00:23, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.56G/9.98G [00:53<00:33, 134MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.63G/9.98G [00:53<00:24, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.74G/9.98G [00:53<00:19, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.78G/9.98G [00:53<00:21, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.82G/9.98G [00:53<00:21, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.88G/9.98G [00:54<00:23, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.95G/9.98G [00:54<00:25, 157MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.99G/9.98G [00:55<00:23, 173MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 6.02G/9.98G [00:55<00:30, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.08G/9.98G [00:56<00:40, 96.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.09G/9.98G [00:56<00:45, 84.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.14G/9.98G [00:57<00:44, 86.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.18G/9.98G [00:57<00:38, 98.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.25G/9.98G [00:58<00:33, 111MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.28G/9.98G [00:58<00:42, 87.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.29G/9.98G [00:58<00:43, 84.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.35G/9.98G [00:59<00:29, 122MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.39G/9.98G [00:59<00:37, 96.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.42G/9.98G [00:59<00:31, 112MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.56G/9.98G [01:00<00:16, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.58G/9.98G [01:00<00:21, 158MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.63G/9.98G [01:00<00:17, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.67G/9.98G [01:01<00:28, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.69G/9.98G [01:02<00:40, 80.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.75G/9.98G [01:02<00:27, 117MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.82G/9.98G [01:02<00:19, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.85G/9.98G [01:02<00:17, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.94G/9.98G [01:03<00:17, 172MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 7.02G/9.98G [01:03<00:15, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.06G/9.98G [01:03<00:14, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.15G/9.98G [01:03<00:10, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.22G/9.98G [01:03<00:09, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.29G/9.98G [01:04<00:07, 352MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.39G/9.98G [01:04<00:05, 446MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.51G/9.98G [01:04<00:04, 594MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.61G/9.98G [01:04<00:03, 671MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.72G/9.98G [01:04<00:02, 760MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.86G/9.98G [01:04<00:02, 909MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.96G/9.98G [01:04<00:02, 800MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.19G/9.98G [01:05<00:02, 620MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.29G/9.98G [01:05<00:02, 600MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.37G/9.98G [01:05<00:02, 608MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.52G/9.98G [01:05<00:02, 717MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.72G/9.98G [01:05<00:01, 902MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.90G/9.98G [01:05<00:01, 1.03GB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.07G/9.98G [01:06<00:00, 1.14GB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.20G/9.98G [01:06<00:00, 1.03GB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.38G/9.98G [01:06<00:00, 730MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.61G/9.98G [01:06<00:00, 936MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.81G/9.98G [01:06<00:00, 1.10GB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.98G/9.98G [01:07<00:00, 148MB/s]\n",
            "Fetching 2 files: 100% 2/2 [01:07<00:00, 33.92s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:08<00:00,  4.18s/it]\n",
            "generation_config.json: 100% 188/188 [00:00<00:00, 928kB/s]\n",
            "Freezing LLAMA!\n",
            "Finish loading LLAMA!\n",
            "path: output/expla_graphs/model_name_inference_llm_llm_model_name_7b_chat_llm_frozen_True_max_txt_len_0_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0.csv\n",
            "  0% 0/35 [00:00<?, ?it/s]/content/knowledge-rag/src/model/llm.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast(dtype=dtype)\n",
            "100% 35/35 [01:10<00:00,  1.98s/it]Test Acc 0.6299638989169675\n",
            "100% 35/35 [01:10<00:00,  2.00s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mexpla_graphs_inference_llm_seed0\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251112_054032-9cm6lsm1/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --dataset expla_graphs --model_name inference_llm --llm_model_name 7b_chat --seed 0"
      ],
      "metadata": {
        "id": "zAtmwrZTRQfI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938e26ed-0dda-4af4-c260-c9127a953d06"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-12 05:43:17.959648: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-12 05:43:17.977841: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762926197.999897    9816 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762926198.006560    9816 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762926198.023139    9816 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762926198.023169    9816 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762926198.023172    9816 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762926198.023175    9816 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-12 05:43:18.028121: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlongsail\u001b[0m (\u001b[33mlongsail-stanford-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/knowledge-rag/wandb/run-20251112_054324-b7xkyotv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexpla_graphs_inference_llm_seed0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/longsail-stanford-university/project_g_retriever\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/longsail-stanford-university/project_g_retriever/runs/b7xkyotv\u001b[0m\n",
            "Namespace(model_name='inference_llm', project='project_g_retriever', seed=0, dataset='expla_graphs', lr=1e-05, wd=0.05, patience=2, batch_size=8, grad_steps=2, num_epochs=10, warmup_epochs=1, eval_batch_size=16, llm_model_name='7b_chat', llm_model_path='', llm_frozen='True', llm_num_virtual_tokens=10, output_dir='output', max_txt_len=512, max_new_tokens=32, max_memory=[80], gnn_model_name='gt', gnn_num_layers=4, gnn_in_dim=1024, gnn_hidden_dim=1024, gnn_num_heads=4, gnn_dropout=0.0)\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "CUDA device count: 1\n",
            "Device 0: NVIDIA A100-SXM4-80GB\n",
            "Loading model: inference_llm\n",
            "Loading LLAMA\n",
            "kwargs: {'max_memory': {0: '80GiB'}, 'device_map': 'auto', 'revision': 'main'}\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.24s/it]\n",
            "Freezing LLAMA!\n",
            "Finish loading LLAMA!\n",
            "path: output/expla_graphs/model_name_inference_llm_llm_model_name_7b_chat_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0.csv\n",
            "  0% 0/35 [00:00<?, ?it/s]/content/knowledge-rag/src/model/llm.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast(dtype=dtype)\n",
            "100% 35/35 [01:19<00:00,  2.22s/it]Test Acc 0.3285198555956679\n",
            "100% 35/35 [01:19<00:00,  2.28s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mexpla_graphs_inference_llm_seed0\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251112_054324-b7xkyotv/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset expla_graphs --model_name pt_llm --seed 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSKcy8ppsxBN",
        "outputId": "b1a66f5d-0088-4d5c-de09-82b351b587fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-12 05:45:04.347273: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-12 05:45:04.366478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762926304.388429   10367 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762926304.395091   10367 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762926304.412334   10367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762926304.412375   10367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762926304.412379   10367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762926304.412390   10367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-12 05:45:04.417726: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlongsail\u001b[0m (\u001b[33mlongsail-stanford-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run e4akz2m7 (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/knowledge-rag/wandb/run-20251112_054511-e4akz2m7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexpla_graphs_pt_llm_7b_seed0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/longsail-stanford-university/project_g_retriever\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/longsail-stanford-university/project_g_retriever/runs/e4akz2m7\u001b[0m\n",
            "Namespace(model_name='pt_llm', project='project_g_retriever', seed=0, dataset='expla_graphs', lr=1e-05, wd=0.05, patience=2, batch_size=8, grad_steps=2, num_epochs=10, warmup_epochs=1, eval_batch_size=16, llm_model_name='7b', llm_model_path='', llm_frozen='True', llm_num_virtual_tokens=10, output_dir='output', max_txt_len=512, max_new_tokens=32, max_memory=[80], gnn_model_name='gt', gnn_num_layers=4, gnn_in_dim=1024, gnn_hidden_dim=1024, gnn_num_heads=4, gnn_dropout=0.0)\n",
            "Loading LLAMA\n",
            "tokenizer_config.json: 100% 776/776 [00:00<00:00, 2.46MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 978kB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 1.59MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 7.41MB/s]\n",
            "config.json: 100% 609/609 [00:00<00:00, 2.39MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 36.2MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 18.0k/9.98G [00:00<118:27:07, 23.4kB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 25.8M/9.98G [00:00<04:05, 40.5MB/s]    \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 1.92M/3.50G [00:01<34:06, 1.71MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   1% 26.6M/3.50G [00:01<01:58, 29.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   1% 49.1M/3.50G [00:01<01:06, 51.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 42.2M/9.98G [00:01<07:23, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 856M/9.98G [00:02<00:13, 682MB/s]  \u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.07G/9.98G [00:02<00:11, 777MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.27G/9.98G [00:02<00:11, 758MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.43G/9.98G [00:02<00:11, 756MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   2% 71.0M/3.50G [00:02<02:19, 24.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.56G/9.98G [00:02<00:11, 751MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  29% 1.02G/3.50G [00:03<00:03, 632MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.68G/9.98G [00:03<00:10, 790MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.79G/9.98G [00:03<00:10, 799MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.95G/9.98G [00:03<00:08, 930MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.09G/9.98G [00:03<00:07, 999MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.24G/9.98G [00:03<00:07, 1.03GB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.37G/9.98G [00:03<00:07, 1.08GB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.54G/9.98G [00:03<00:06, 1.16GB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.67G/9.98G [00:03<00:06, 1.13GB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.79G/9.98G [00:04<00:10, 665MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  38% 1.33G/3.50G [00:04<00:06, 330MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.88G/9.98G [00:05<00:23, 303MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.00G/9.98G [00:05<00:25, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.07G/9.98G [00:05<00:22, 302MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.17G/9.98G [00:06<00:19, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.25G/9.98G [00:06<00:20, 332MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  44% 1.55G/3.50G [00:06<00:07, 260MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.30G/9.98G [00:06<00:20, 320MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.35G/9.98G [00:06<00:30, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.42G/9.98G [00:07<00:26, 243MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  48% 1.69G/3.50G [00:07<00:07, 236MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.49G/9.98G [00:07<00:24, 266MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.56G/9.98G [00:07<00:26, 240MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  52% 1.81G/3.50G [00:07<00:07, 230MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.61G/9.98G [00:09<01:22, 76.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.72G/9.98G [00:10<00:50, 124MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  55% 1.91G/3.50G [00:10<00:12, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  57% 2.00G/3.50G [00:10<00:10, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  61% 2.13G/3.50G [00:10<00:07, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  64% 2.23G/3.50G [00:10<00:05, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  67% 2.35G/3.50G [00:10<00:03, 306MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  70% 2.44G/3.50G [00:11<00:03, 299MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  72% 2.51G/3.50G [00:11<00:03, 319MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  75% 2.61G/3.50G [00:11<00:03, 267MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  76% 2.67G/3.50G [00:13<00:06, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  78% 2.74G/3.50G [00:14<00:07, 101MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.82G/9.98G [00:14<02:05, 49.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  82% 2.87G/3.50G [00:14<00:04, 157MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.45G/9.98G [00:14<00:27, 198MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  86% 3.02G/3.50G [00:14<00:02, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  90% 3.14G/3.50G [00:14<00:01, 318MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  93% 3.27G/3.50G [00:14<00:00, 416MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.67G/9.98G [00:14<00:22, 235MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  98% 3.43G/3.50G [00:15<00:00, 324MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 3.50G/3.50G [00:15<00:00, 220MB/s]\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 4.95G/9.98G [00:16<00:20, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.06G/9.98G [00:18<00:36, 133MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.16G/9.98G [00:18<00:30, 160MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.32G/9.98G [00:18<00:20, 228MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.44G/9.98G [00:18<00:16, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.56G/9.98G [00:18<00:12, 344MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.69G/9.98G [00:18<00:09, 431MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.84G/9.98G [00:19<00:07, 546MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.95G/9.98G [00:19<00:06, 636MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.11G/9.98G [00:19<00:05, 729MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.24G/9.98G [00:19<00:04, 819MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.41G/9.98G [00:20<00:06, 526MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.51G/9.98G [00:20<00:07, 449MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.58G/9.98G [00:20<00:07, 428MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.66G/9.98G [00:20<00:07, 457MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.72G/9.98G [00:20<00:09, 359MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.80G/9.98G [00:21<00:12, 261MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.86G/9.98G [00:21<00:10, 297MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.90G/9.98G [00:22<00:18, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.96G/9.98G [00:22<00:18, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.05G/9.98G [00:22<00:12, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.15G/9.98G [00:23<00:09, 291MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.23G/9.98G [00:23<00:07, 347MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.40G/9.98G [00:23<00:04, 533MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.52G/9.98G [00:23<00:03, 620MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.66G/9.98G [00:23<00:03, 706MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.75G/9.98G [00:24<00:05, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.81G/9.98G [00:24<00:10, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.88G/9.98G [00:25<00:14, 147MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.96G/9.98G [00:26<00:16, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.03G/9.98G [00:26<00:12, 158MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.19G/9.98G [00:27<00:07, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.28G/9.98G [00:27<00:05, 308MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.34G/9.98G [00:27<00:04, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.44G/9.98G [00:27<00:04, 329MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.50G/9.98G [00:27<00:04, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.59G/9.98G [00:28<00:03, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.63G/9.98G [00:28<00:03, 352MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.70G/9.98G [00:28<00:04, 317MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.74G/9.98G [00:28<00:05, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.78G/9.98G [00:28<00:05, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.84G/9.98G [00:29<00:04, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.90G/9.98G [00:29<00:03, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.96G/9.98G [00:29<00:03, 318MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 9.02G/9.98G [00:29<00:02, 355MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.08G/9.98G [00:29<00:02, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.14G/9.98G [00:29<00:02, 413MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.20G/9.98G [00:29<00:01, 434MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.33G/9.98G [00:30<00:01, 619MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.51G/9.98G [00:30<00:00, 891MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.72G/9.98G [00:30<00:00, 1.12GB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.98G/9.98G [00:30<00:00, 326MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:30<00:00, 15.43s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.00s/it]\n",
            "generation_config.json: 100% 188/188 [00:00<00:00, 853kB/s]\n",
            "Freezing LLAMA!\n",
            "Finish loading LLAMA!\n",
            "trainable params: 40960 || all params: 6738456576 || trainable%: 0.0006078543289257816\n",
            "  0% 0/2070 [00:00<?, ?it/s]/content/knowledge-rag/src/model/pt_llm.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast(dtype=dtype)\n",
            " 10% 207/2070 [01:35<14:40,  2.12it/s]Epoch: 0|10: Train Loss (Epoch Mean): 2.7635669351199974\n",
            "Epoch: 0|10: Val Loss: 2.224963048526219\n",
            "Saving checkpoint at epoch 0 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 0 Val Loss 2.224963048526219 Best Val Loss 2.224963048526219 Best Epoch 0\n",
            " 20% 414/2070 [03:27<12:43,  2.17it/s]Epoch: 1|10: Train Loss (Epoch Mean): 1.4198940456777378\n",
            "Epoch: 1|10: Val Loss: 0.8440640577248164\n",
            "Saving checkpoint at epoch 1 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 1 Val Loss 0.8440640577248164 Best Val Loss 0.8440640577248164 Best Epoch 1\n",
            " 30% 621/2070 [05:19<11:14,  2.15it/s]Epoch: 2|10: Train Loss (Epoch Mean): 0.5549852794783127\n",
            "Epoch: 2|10: Val Loss: 0.4620454660483769\n",
            "Saving checkpoint at epoch 2 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 2 Val Loss 0.4620454660483769 Best Val Loss 0.4620454660483769 Best Epoch 2\n",
            " 40% 828/2070 [07:11<09:43,  2.13it/s]Epoch: 3|10: Train Loss (Epoch Mean): 0.3759824197361435\n",
            "Epoch: 3|10: Val Loss: 0.40785275101661683\n",
            "Saving checkpoint at epoch 3 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 3 Val Loss 0.40785275101661683 Best Val Loss 0.40785275101661683 Best Epoch 3\n",
            " 50% 1035/2070 [09:04<08:09,  2.12it/s]Epoch: 4|10: Train Loss (Epoch Mean): 0.34546684621324864\n",
            "Epoch: 4|10: Val Loss: 0.3981825917959213\n",
            "Saving checkpoint at epoch 4 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 4 Val Loss 0.3981825917959213 Best Val Loss 0.3981825917959213 Best Epoch 4\n",
            " 60% 1242/2070 [10:56<06:21,  2.17it/s]Epoch: 5|10: Train Loss (Epoch Mean): 0.3334977811085429\n",
            "Epoch: 5|10: Val Loss: 0.38445247837475366\n",
            "Saving checkpoint at epoch 5 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 5 Val Loss 0.38445247837475366 Best Val Loss 0.38445247837475366 Best Epoch 5\n",
            " 70% 1449/2070 [12:48<04:49,  2.15it/s]Epoch: 6|10: Train Loss (Epoch Mean): 0.32624199921670166\n",
            "Epoch: 6|10: Val Loss: 0.3861468151211739\n",
            "Epoch 6 Val Loss 0.3861468151211739 Best Val Loss 0.38445247837475366 Best Epoch 5\n",
            " 80% 1656/2070 [14:39<03:08,  2.20it/s]Epoch: 7|10: Train Loss (Epoch Mean): 0.31799522798130475\n",
            "Epoch: 7|10: Val Loss: 0.37355760782957076\n",
            "Saving checkpoint at epoch 7 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 7 Val Loss 0.37355760782957076 Best Val Loss 0.37355760782957076 Best Epoch 7\n",
            " 90% 1863/2070 [16:31<01:36,  2.14it/s]Epoch: 8|10: Train Loss (Epoch Mean): 0.31291918499746185\n",
            "Epoch: 8|10: Val Loss: 0.36734245866537096\n",
            "Saving checkpoint at epoch 8 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 8 Val Loss 0.36734245866537096 Best Val Loss 0.36734245866537096 Best Epoch 8\n",
            "100% 2070/2070 [18:23<00:00,  2.18it/s]Epoch: 9|10: Train Loss (Epoch Mean): 0.3083411548736591\n",
            "Epoch: 9|10: Val Loss: 0.3628337017127446\n",
            "Saving checkpoint at epoch 9 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 9 Val Loss 0.3628337017127446 Best Val Loss 0.3628337017127446 Best Epoch 9\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "path: output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0.csv\n",
            "Loading checkpoint from output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 1/35 [00:00<00:26,  1.29it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:21,  1.54it/s]\u001b[A\n",
            "  9% 3/35 [00:01<00:20,  1.56it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:26,  1.17it/s]\u001b[A\n",
            " 14% 5/35 [00:03<00:25,  1.18it/s]\u001b[A\n",
            " 17% 6/35 [00:04<00:22,  1.31it/s]\u001b[A\n",
            " 20% 7/35 [00:05<00:19,  1.42it/s]\u001b[A\n",
            " 23% 8/35 [00:05<00:17,  1.51it/s]\u001b[A\n",
            " 26% 9/35 [00:06<00:17,  1.51it/s]\u001b[A\n",
            " 29% 10/35 [00:07<00:16,  1.52it/s]\u001b[A\n",
            " 31% 11/35 [00:07<00:15,  1.53it/s]\u001b[A\n",
            " 34% 12/35 [00:08<00:14,  1.54it/s]\u001b[A\n",
            " 37% 13/35 [00:08<00:14,  1.55it/s]\u001b[A\n",
            " 40% 14/35 [00:10<00:22,  1.06s/it]\u001b[A\n",
            " 43% 15/35 [00:11<00:18,  1.09it/s]\u001b[A\n",
            " 46% 16/35 [00:12<00:15,  1.21it/s]\u001b[A\n",
            " 49% 17/35 [00:12<00:13,  1.34it/s]\u001b[A\n",
            " 51% 18/35 [00:13<00:13,  1.25it/s]\u001b[A\n",
            " 54% 19/35 [00:14<00:12,  1.30it/s]\u001b[A\n",
            " 57% 20/35 [00:15<00:11,  1.25it/s]\u001b[A\n",
            " 60% 21/35 [00:15<00:10,  1.31it/s]\u001b[A\n",
            " 63% 22/35 [00:16<00:09,  1.41it/s]\u001b[A\n",
            " 66% 23/35 [00:17<00:08,  1.45it/s]\u001b[A\n",
            " 69% 24/35 [00:17<00:07,  1.54it/s]\u001b[A\n",
            " 71% 25/35 [00:18<00:06,  1.54it/s]\u001b[A\n",
            " 74% 26/35 [00:19<00:05,  1.54it/s]\u001b[A\n",
            " 77% 27/35 [00:19<00:05,  1.57it/s]\u001b[A\n",
            " 80% 28/35 [00:20<00:04,  1.61it/s]\u001b[A\n",
            " 83% 29/35 [00:21<00:04,  1.46it/s]\u001b[A\n",
            " 86% 30/35 [00:21<00:03,  1.55it/s]\u001b[A\n",
            " 89% 31/35 [00:22<00:02,  1.60it/s]\u001b[A\n",
            " 91% 32/35 [00:22<00:01,  1.60it/s]\u001b[A\n",
            " 94% 33/35 [00:23<00:01,  1.58it/s]\u001b[A\n",
            " 97% 34/35 [00:24<00:00,  1.59it/s]\u001b[A\n",
            "100% 35/35 [00:24<00:00,  1.82it/s]\u001b[ATest Acc 0.5776173285198556\n",
            "100% 2070/2070 [19:04<00:00,  1.81it/s]\n",
            "100% 35/35 [00:24<00:00,  1.43it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mexpla_graphs_pt_llm_7b_seed0\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251112_054511-e4akz2m7/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset expla_graphs --model_name graph_llm --seed 0 --llm_model_name 1b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7a2_n0C5K2l",
        "outputId": "e1da623f-8040-4cd7-dfcb-cd96b20c5449"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-12 06:22:31.848213: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-12 06:22:31.867251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762928551.890308   20033 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762928551.897168   20033 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762928551.914382   20033 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762928551.914425   20033 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762928551.914428   20033 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762928551.914431   20033 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-12 06:22:31.919522: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlongsail\u001b[0m (\u001b[33mlongsail-stanford-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run h7az5uou (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/knowledge-rag/wandb/run-20251112_062238-h7az5uou\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexpla_graphs_graph_llm_1b_seed0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/longsail-stanford-university/project_g_retriever\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/longsail-stanford-university/project_g_retriever/runs/h7az5uou\u001b[0m\n",
            "Namespace(model_name='graph_llm', project='project_g_retriever', seed=0, dataset='expla_graphs', lr=1e-05, wd=0.05, patience=2, batch_size=8, grad_steps=2, num_epochs=10, warmup_epochs=1, eval_batch_size=16, llm_model_name='1b', llm_model_path='', llm_frozen='True', llm_num_virtual_tokens=10, output_dir='output', max_txt_len=512, max_new_tokens=32, max_memory=[80], gnn_model_name='gt', gnn_num_layers=4, gnn_in_dim=1024, gnn_hidden_dim=1024, gnn_num_heads=4, gnn_dropout=0.0)\n",
            "Loading LLAMA\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Freezing LLAMA!\n",
            "Finish loading LLAMA!\n",
            "trainable params: 27289600 || all params: 1263104000 || trainable%: 2.1605188488042155\n",
            "  0% 0/2070 [00:00<?, ?it/s]/content/knowledge-rag/src/model/graph_llm.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast(dtype=dtype)\n",
            " 10% 206/2070 [00:19<02:50, 10.95it/s]Epoch: 0|10: Train Loss (Epoch Mean): 1.610876985434173\n",
            "Epoch: 0|10: Val Loss: 0.17907866729157312\n",
            "Saving checkpoint at epoch 0 to output/expla_graphs/model_name_graph_llm_llm_model_name_1b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 0 Val Loss 0.17907866729157312 Best Val Loss 0.17907866729157312 Best Epoch 0\n",
            " 20% 414/2070 [00:44<02:34, 10.68it/s]Epoch: 1|10: Train Loss (Epoch Mean): 0.1814314436149482\n",
            "Epoch: 1|10: Val Loss: 0.17581033131905965\n",
            "Saving checkpoint at epoch 1 to output/expla_graphs/model_name_graph_llm_llm_model_name_1b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 1 Val Loss 0.17581033131905965 Best Val Loss 0.17581033131905965 Best Epoch 1\n",
            " 30% 620/2070 [01:08<02:11, 11.06it/s]Epoch: 2|10: Train Loss (Epoch Mean): 0.17519015700056934\n",
            "Epoch: 2|10: Val Loss: 0.17300571181944438\n",
            "Saving checkpoint at epoch 2 to output/expla_graphs/model_name_graph_llm_llm_model_name_1b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Epoch 2 Val Loss 0.17300571181944438 Best Val Loss 0.17300571181944438 Best Epoch 2\n",
            " 40% 828/2070 [01:33<01:58, 10.46it/s]Epoch: 3|10: Train Loss (Epoch Mean): 0.1647028303232746\n",
            "Epoch: 3|10: Val Loss: 0.18212578530822482\n",
            "Epoch 3 Val Loss 0.18212578530822482 Best Val Loss 0.17300571181944438 Best Epoch 2\n",
            " 50% 1034/2070 [01:56<01:35, 10.83it/s]Epoch: 4|10: Train Loss (Epoch Mean): 0.15885413272513282\n",
            "Epoch: 4|10: Val Loss: 0.17531943800193922\n",
            "Epoch 4 Val Loss 0.17531943800193922 Best Val Loss 0.17300571181944438 Best Epoch 2\n",
            "Early stop at epoch 4\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "path: output/expla_graphs/model_name_graph_llm_llm_model_name_1b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0.csv\n",
            "Loading checkpoint from output/expla_graphs/model_name_graph_llm_llm_model_name_1b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            "  3% 1/35 [00:01<00:36,  1.06s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            "  6% 2/35 [00:02<00:33,  1.03s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            "  9% 3/35 [00:03<00:32,  1.02s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 11% 4/35 [00:04<00:31,  1.02s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 14% 5/35 [00:05<00:30,  1.01s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 17% 6/35 [00:06<00:29,  1.01s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 20% 7/35 [00:07<00:28,  1.00s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 23% 8/35 [00:08<00:27,  1.00s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 26% 9/35 [00:09<00:26,  1.00s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 50% 1035/2070 [02:10<01:35, 10.83it/s]\n",
            " 29% 10/35 [00:10<00:25,  1.01s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 31% 11/35 [00:11<00:24,  1.00s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 34% 12/35 [00:12<00:23,  1.00s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 37% 13/35 [00:13<00:22,  1.00s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 40% 14/35 [00:14<00:23,  1.13s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 43% 15/35 [00:15<00:21,  1.09s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 46% 16/35 [00:16<00:20,  1.06s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 49% 17/35 [00:17<00:18,  1.05s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 51% 18/35 [00:18<00:17,  1.03s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 54% 19/35 [00:19<00:16,  1.02s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 57% 20/35 [00:20<00:15,  1.02s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 60% 21/35 [00:21<00:14,  1.02s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 63% 22/35 [00:22<00:13,  1.02s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 66% 23/35 [00:23<00:12,  1.01s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 69% 24/35 [00:24<00:11,  1.01s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 71% 25/35 [00:25<00:10,  1.00s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 74% 26/35 [00:26<00:09,  1.00s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 77% 27/35 [00:27<00:08,  1.01s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 80% 28/35 [00:28<00:07,  1.01s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 83% 29/35 [00:29<00:06,  1.00s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 86% 30/35 [00:30<00:05,  1.00s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 89% 31/35 [00:31<00:03,  1.00it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 91% 32/35 [00:32<00:02,  1.00it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 94% 33/35 [00:33<00:01,  1.00it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            " 97% 34/35 [00:34<00:01,  1.00s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            "100% 35/35 [00:35<00:00,  1.01it/s]\u001b[ATest Acc 0.5342960288808665\n",
            " 50% 1035/2070 [02:36<02:36,  6.63it/s]\n",
            "100% 35/35 [00:35<00:00,  1.02s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mexpla_graphs_graph_llm_1b_seed0\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251112_062238-h7az5uou/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63KPmJQzBGyY",
        "outputId": "48e3a168-fb22-46df-9bf3-4f74935d4bad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects:  25% (1/4)\u001b[K\rremote: Compressing objects:  50% (2/4)\u001b[K\rremote: Compressing objects:  75% (3/4)\u001b[K\rremote: Compressing objects: 100% (4/4)\u001b[K\rremote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 5 (delta 3), reused 3 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  20% (1/5)\rUnpacking objects:  40% (2/5)\rUnpacking objects:  60% (3/5)\rUnpacking objects:  80% (4/5)\rUnpacking objects: 100% (5/5)\rUnpacking objects: 100% (5/5), 483 bytes | 483.00 KiB/s, done.\n",
            "From https://github.com/wenovus/knowledge-rag\n",
            "   6fdd185..cafee6f  main       -> origin/main\n",
            "Updating 6fdd185..cafee6f\n",
            "Fast-forward\n",
            " src/model/graph_llm.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZGaJLCZIuSvt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}